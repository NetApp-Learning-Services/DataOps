{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning and quantization\n",
    "\n",
    "In this example, you will fine-tune a small language model (DistilBERT in this case) in a sentiment classification example and then quantizing it from FP32 to INT8. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install transformers==4.41.2\n",
    "%pip install datasets==2.20.2\n",
    "%pip install numpy==1.26.3\n",
    "%pip install pandas==2.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Next, you will check whether there are any NVIDIA GPUs configured in the environment.  There are not but check anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will load and prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a custom TextDataset class  (which is a custom implementation of the PyTorch's Dataset class).  \n",
    "\n",
    "The DataLoader is a crucial part of the PyTorch training pipeline. It:\n",
    "\n",
    "* Batches the data, which allows for more efficient processing.\n",
    "* Shuffles the data, which helps in reducing overfitting.\n",
    "* Handles the conversion of your data into PyTorch tensors.\n",
    "* Can distribute the data across multiple CPU cores for faster loading (though in this CPU-only version, we're not using multiple cores).\n",
    "\n",
    "When we use this train_loader in our training loop, it will yield batches of data, each containing 8 samples (except possibly the last batch if the dataset size isn't divisible by 4). Each batch will be a dictionary with keys 'input_ids', 'attention_mask', and 'label', where each value is a tensor of shape (8, ...).\n",
    "\n",
    "This setup allows for efficient, batched processing of our dataset during training, which is crucial for handling larger datasets and speeding up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    #The constructor takes the tokenized dataset as an argument and stores it\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset  \n",
    "\n",
    "    #Returns the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset) \n",
    "\n",
    "    # Fetches a single item from the dataset\n",
    "    # Takes an index (idx)\n",
    "    # Returns a dictionary containing:\n",
    "    #      input_ids: the tokenzied and encoded text\n",
    "    #      attention_mask: a mask indicating which tokens are padding and which are actual input\n",
    "    #      label: label for the text (in this case, the sentiment)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.tokenized_dataset[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids']),\n",
    "            'attention_mask': torch.tensor(item['attention_mask']),\n",
    "            'label': torch.tensor(item['label'])\n",
    "        }\n",
    "\n",
    "# Creates an intance of the custom dataset class\n",
    "train_dataset = TextDataset(tokenized_dataset)\n",
    "\n",
    "# Creates a PyTorch utility for loading data in batches of 8 items and sheffle the data before each epoch (to prevent model from learning the order of the data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}, Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a calibration function\n",
    "def calibrate(model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            _ = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Prepare the model for static quantization\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "# Calibrate the model\n",
    "calibrate(model, train_loader)\n",
    "\n",
    "# Convert the model to quantized version\n",
    "quantized_model = torch.quantization.convert(model, inplace=False)\n",
    "\n",
    "print(\"Quantization complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate model size\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "original_size = get_model_size(model)\n",
    "quantized_size = get_model_size(quantized_model)\n",
    "\n",
    "print(f\"Original model size: {original_size:.2f} MB\")\n",
    "print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "print(f\"Size reduction: {(1 - quantized_size/original_size)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Evaluation Function\n",
    "def evaluate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "print(\"\\nAccuracy Comparison:\")\n",
    "original_accuracy = evaluate_accuracy(model, val_loader)\n",
    "quantized_accuracy = evaluate_accuracy(quantized_model, val_loader)\n",
    "\n",
    "print(f\"Original model accuracy: {original_accuracy:.4f}\")\n",
    "print(f\"Quantized model accuracy: {quantized_accuracy:.4f}\")\n",
    "print(f\"Accuracy change: {(quantized_accuracy - original_accuracy)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference time comparison\n",
    "input_text = \"This movie was fantastic! I really enjoyed it.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    original_output = model(**inputs).logits\n",
    "    original_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    quantized_output = quantized_model(**inputs).logits\n",
    "    quantized_time = time.time() - start_time\n",
    "\n",
    "print(\"Original model output:\", original_output)\n",
    "print(f\"Original model inference time: {original_time:.4f} seconds\")\n",
    "print(\"Quantized model output:\", quantized_output)\n",
    "print(f\"Quantized model inference time: {quantized_time:.4f} seconds\")\n",
    "print(f\"Speedup: {original_time/quantized_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1)\n",
    "    \n",
    "    return \"Positive\" if prediction.item() == 1 else \"Negative\"\n",
    "\n",
    "test_texts = [\n",
    "    \"This movie was fantastic! I really enjoyed it.\",\n",
    "    \"I've never been so bored in my life. Terrible film.\",\n",
    "    \"The acting was okay, but the plot was confusing.\"\n",
    "]\n",
    "\n",
    "print(\"\\nExample Predictions:\")\n",
    "for text in test_texts:\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Original model prediction: {predict_sentiment(model, text)}\")\n",
    "    print(f\"Quantized model prediction: {predict_sentiment(quantized_model, text)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
