{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy-loading DistilBERT\n",
    "\n",
    "An example using a small language model from the Hugging Face Transformers library that can run on CPUs. We'll use the \"distilbert-base-uncased\" model, which is a smaller version of BERT. This example will demonstrate how to separate the model architecture from its weights and load them on demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install transformers==4.41.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading weights\n",
    "\n",
    "This example demonstrates the basic concept of separating model architecture from weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_weights(model, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model_weights(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Initialize model configuration\n",
    "config = DistilBertConfig.from_pretrained('distilbert-base-uncased')\n",
    "config.num_labels = 2  # Binary classification\n",
    "\n",
    "# Create model architecture\n",
    "model = DistilBertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights (simulating a pre-trained model)\n",
    "save_model_weights(model, \"weights/distilbert_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_lazy():\n",
    "    # Create the model architecture (this is fast and lightweight)\n",
    "    model = DistilBertForSequenceClassification(config)\n",
    "    \n",
    "    # Load weights only when needed (this is the potentially slower part)\n",
    "    return load_model_weights(model, \"weights/distilbert_weights.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_full():\n",
    "    # Load the full model including weights\n",
    "    return DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run inference\n",
    "def run_inference(model, text):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get prediction\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach allows you to keep the model architecture definition in your code while storing the weights separately. The weights are only loaded when you actually need to run inference.\n",
    "\n",
    "In a production environment, you might want to:\n",
    "\n",
    "* Implement caching to avoid loading the weights for every inference.\n",
    "* Use more efficient storage formats for the weights.\n",
    "* Implement error handling and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing comparison\n",
    "sample_text = \"I love using this language model. It's fantastic!\"\n",
    "\n",
    "print(\"\\nFull loading approach:\")\n",
    "start_time = time.time()\n",
    "model_full = get_model_full()\n",
    "full_load_time = time.time() - start_time\n",
    "print(f\"  Model loading time: {full_load_time:.4f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "result_full = run_inference(model_full, sample_text)\n",
    "full_inference_time = time.time() - start_time\n",
    "print(f\"  Inference time: {full_inference_time:.4f} seconds\")\n",
    "print(f\"  Total time: {full_load_time + full_inference_time:.4f} seconds\")\n",
    "print(f\"  Result: {result_full}\")\n",
    "\n",
    "print(\"Lazy loading approach:\")\n",
    "start_time = time.time()\n",
    "model_lazy = get_model_lazy()\n",
    "lazy_load_time = time.time() - start_time\n",
    "print(f\"  Model loading time: {lazy_load_time:.4f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "result_lazy = run_inference(model_lazy, sample_text)\n",
    "lazy_inference_time = time.time() - start_time\n",
    "print(f\"  Inference time: {lazy_inference_time:.4f} seconds\")\n",
    "print(f\"  Total time: {lazy_load_time + lazy_inference_time:.4f} seconds\")\n",
    "print(f\"  Result: {result_lazy}\")\n",
    "\n",
    "# Print number of parameters\n",
    "num_params = sum(p.numel() for p in model_lazy.parameters())\n",
    "print(f\"\\nNumber of parameters in the model: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what to expect:\n",
    "\n",
    "* The lazy loading approach will likely have a faster initial load time (when creating the model architecture) but might be slightly slower when running inference for the first time (as it needs to load the weights).\n",
    "* The full loading approach will likely take longer to load initially but might have a slightly faster inference time.\n",
    "* The number of parameters will give you an idea of the model's size. DistilBERT typically has around 66 million parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
