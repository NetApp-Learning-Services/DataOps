{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning and quantization\n",
    "\n",
    "In this example, you will fine-tune a small language model (GPT-2 in this case) and then quantizing it from FP32 to INT8. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install transformers==4.30.2\n",
    "%pip install datasets==2.13.1\n",
    "%pip install numpy==1.24.3\n",
    "%pip install pandas==2.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Next, you will check whether there are any NVIDIA GPUs configured in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will load and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small dataset (e.g., a subset of WikiText-2)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a custom TextDataset class  (which is a custom implementation of the PyTorch's Dataset class).  \n",
    "\n",
    "The DataLoader is a crucial part of the PyTorch training pipeline. It:\n",
    "\n",
    "* Batches the data, which allows for more efficient processing.\n",
    "* Shuffles the data, which helps in reducing overfitting.\n",
    "* Handles the conversion of your data into PyTorch tensors.\n",
    "* Can distribute the data across multiple CPU cores for faster loading (though in this CPU-only version, we're not using multiple cores).\n",
    "\n",
    "When we use this train_loader in our training loop, it will yield batches of data, each containing 4 samples (except possibly the last batch if the dataset size isn't divisible by 4). Each batch will be a dictionary with keys 'input_ids' and 'attention_mask', where each value is a tensor of shape (4, ...).\n",
    "\n",
    "This setup allows for efficient, batched processing of our dataset during training, which is crucial for handling larger datasets and speeding up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    #The constructor takes the tokenized dataset as an argument and stores it\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset  \n",
    "\n",
    "    #Returns the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset) \n",
    "\n",
    "    # Fetches a single item from the dataset\n",
    "    # Takes an index (idx)\n",
    "    # Returns a dictionary containing:\n",
    "    #      input_ids: the tokenzied and encoded text\n",
    "    #      attention_mask: a mask indicating which tokens are padding and which are actual input\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.tokenized_dataset[idx]\n",
    "        return torch.tensor(item['input_ids']), torch.tensor(item['attention_mask'])\n",
    "\n",
    "# Creates an intance of the custom dataset class\n",
    "train_dataset = TextDataset(tokenized_dataset)\n",
    "\n",
    "# Creates a PyTorch utility for loading data in batches of 4 items and sheffle the data before each epoch (to prevent model from learning the order of the data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask = batch\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Fine-tuning complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
