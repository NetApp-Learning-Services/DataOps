{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning and quantization\n",
    "\n",
    "In this example, you will fine-tune a small language model (GPT-2 in this case) and then quantizing it from FP32 to INT8. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install transformers==4.41.2\n",
    "%pip install datasets==2.20.2\n",
    "%pip install numpy==1.26.3\n",
    "%pip install pandas==2.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Next, you will check whether there are any NVIDIA GPUs configured in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will load and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small dataset (e.g., a subset of WikiText-2)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a custom TextDataset class  (which is a custom implementation of the PyTorch's Dataset class).  \n",
    "\n",
    "The DataLoader is a crucial part of the PyTorch training pipeline. It:\n",
    "\n",
    "* Batches the data, which allows for more efficient processing.\n",
    "* Shuffles the data, which helps in reducing overfitting.\n",
    "* Handles the conversion of your data into PyTorch tensors.\n",
    "* Can distribute the data across multiple CPU cores for faster loading (though in this CPU-only version, we're not using multiple cores).\n",
    "\n",
    "When we use this train_loader in our training loop, it will yield batches of data, each containing 4 samples (except possibly the last batch if the dataset size isn't divisible by 4). Each batch will be a dictionary with keys 'input_ids' and 'attention_mask', where each value is a tensor of shape (4, ...).\n",
    "\n",
    "This setup allows for efficient, batched processing of our dataset during training, which is crucial for handling larger datasets and speeding up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    #The constructor takes the tokenized dataset as an argument and stores it\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset  \n",
    "\n",
    "    #Returns the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset) \n",
    "\n",
    "    # Fetches a single item from the dataset\n",
    "    # Takes an index (idx)\n",
    "    # Returns a dictionary containing:\n",
    "    #      input_ids: the tokenzied and encoded text\n",
    "    #      attention_mask: a mask indicating which tokens are padding and which are actual input\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.tokenized_dataset[idx]\n",
    "        return torch.tensor(item['input_ids']), torch.tensor(item['attention_mask'])\n",
    "\n",
    "# Creates an intance of the custom dataset class\n",
    "train_dataset = TextDataset(tokenized_dataset)\n",
    "\n",
    "# Creates a PyTorch utility for loading data in batches of 4 items and sheffle the data before each epoch (to prevent model from learning the order of the data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:  This next step can take up to 2 hours to complete the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask = batch\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        end_time = time.time()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}, Time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static quantization example\n",
    "# def calibrate(model, loader):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for batch in loader:\n",
    "#             input_ids, attention_mask = batch\n",
    "#             input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "#             _ = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# # Prepare the model for quantization\n",
    "# model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "# torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "# # Calibrate the model\n",
    "# calibrate(model, train_loader)\n",
    "\n",
    "# # Convert the model to quantized version\n",
    "# quantized_model = torch.quantization.convert(model, inplace=False)\n",
    "\n",
    "# Dynamic quantization example\n",
    "# quantized_model = torch.quantization.quantize_dynamic(\n",
    "#     model, {torch.nn.Linear}, dtype=torch.quint8\n",
    "# )\n",
    "\n",
    "# Custom quantization example\n",
    "# This approach avoids quantizing the embedding layers, where were causing errors\n",
    "def quantize_model(model):\n",
    "    # Quantize only the transformer blocks\n",
    "    for name, module in model.named_children():\n",
    "        if \"h\" in name:  # This is the transformer block in GPT-2\n",
    "            for sub_name, sub_module in module.named_children():\n",
    "                if isinstance(sub_module, nn.Linear):  #Quantizes only the linear layers within these blocks\n",
    "                    module._modules[sub_name] = torch.quantization.quantize_dynamic(\n",
    "                        sub_module, {torch.nn.Linear}, dtype=torch.qint8\n",
    "                    )\n",
    "    return model\n",
    "\n",
    "quantized_model = quantize_model(model)\n",
    "\n",
    "print(\"Quantization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GPT2 model's total parameters: {model.num_parameters()}\")\n",
    "model_param = next(iter(model.state_dict().values()))\n",
    "print(f\"GPT2 model precision (weight data type): {model_param.dtype}\")\n",
    "print(f\"Quantized GPT2 model's total parameters: {quantized_model.num_parameters()}\")\n",
    "quantized_mparam = next(iter(quantized_model.state_dict().values()))\n",
    "print(f\"GPT2 model precision (weight data type): {quantized_mparam.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate model size\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "original_size = get_model_size(model)\n",
    "quantized_size = get_model_size(quantized_model)\n",
    "\n",
    "print(f\"Original model size: {original_size:.2f} MB\")\n",
    "print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "print(f\"Size reduction: {(1 - quantized_size/original_size)*100:.2f}%\")\n",
    "\n",
    "# Inference comparison (example)\n",
    "input_text = \"The quick brown fox\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    original_output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "    quantized_output = quantized_model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(\"Original model output:\")\n",
    "print(tokenizer.decode(original_output[0], skip_special_tokens=True))\n",
    "print(\"\\nQuantized model output:\")\n",
    "print(tokenizer.decode(quantized_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference comparison (example)\n",
    "def generate_text(model, prompt, max_length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"The quick brown fox\",\n",
    "    \"In a world where technology\",\n",
    "    \"Climate change is a pressing issue because\",\n",
    "    \"The future of artificial intelligence\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nInference Comparison:\")\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    \n",
    "    # Original model\n",
    "    start_time = time.time()\n",
    "    original_output = generate_text(model, prompt)\n",
    "    original_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Original model output: {original_output}\")\n",
    "    print(f\"Original model inference time: {original_time:.4f} seconds\")\n",
    "    \n",
    "    # Quantized model\n",
    "    start_time = time.time()\n",
    "    quantized_output = generate_text(quantized_model, prompt)\n",
    "    quantized_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Quantized model output: {quantized_output}\")\n",
    "    print(f\"Quantized model inference time: {quantized_time:.4f} seconds\")\n",
    "    \n",
    "    print(f\"Speedup: {original_time/quantized_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The perplexity comparison test takes about 30 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity comparison\n",
    "def calculate_perplexity(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask = batch\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            total_tokens += torch.sum(attention_mask).item()\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "print(\"\\nPerplexity Comparison:\")\n",
    "original_perplexity = calculate_perplexity(model, train_loader)\n",
    "quantized_perplexity = calculate_perplexity(quantized_model, train_loader)\n",
    "\n",
    "print(f\"Original model perplexity: {original_perplexity:.2f}\")\n",
    "print(f\"Quantized model perplexity: {quantized_perplexity:.2f}\")\n",
    "print(f\"Perplexity increase: {(quantized_perplexity/original_perplexity - 1)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
