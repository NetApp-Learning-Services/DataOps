{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning example\n",
    "\n",
    "A step-by-step guide for fine tuning a FLAN-T5 small language model to identify NetApp products by a description, using the transformers library, and running fast on a regular computer’s CPU.\n",
    "\n",
    "FLAN-T5 is an enhanced version of the T5 (Text-to-Text Transfer Transformer) lanaguage model, developed by Google. It has been fine-tuned on a mixture of unsupervised and supervised tasks, making it a powerful encoder-decoder language model. FLAN-T5 can handle various language tasks, such as translation, coherence checking, sentence similarity assessment, and document summarization. You can use it for both personal and professional purposes. Imagine having an intelligent companion proficient in language understanding, assisting with various text-related tasks you encounter in your work.  This model is an in a class called Small Lanaguage Model (SLM). A SML is a compact AI model that uses a smaller neural network, fewer parameters, and less training data. Unlike large-scale language models (LLMs), SLMs achieve impressive performance while requiring fewer resources and less computing power. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "- Hugging Face: This platform helps you access FLAN-T5 and makes it easy to download and use for fine-tuning.<br>\n",
    "- Transformers: This tool simplifies loading the pre-trained FLAN-T5 model and gives you helpful functions for fine-tuning it.<br>\n",
    "- Datasets: This is a collection of datasets ready to use, which is important for finding the right data to fine-tune FLAN-T5.<br>\n",
    "- Sentencepiece: This tool helps with tokenization, especially for big and multilanguage text data.<br>\n",
    "- Tokenizers: This library helps convert text into a format that works well for your specific task.<br>\n",
    "- Evaluate: You can use this library to check how well your fine-tuned model is performing by measuring various metrics.<br>\n",
    "- Rouge score: This is a specific metric used to see how good the text produced by FLAN-T5 is.<br>\n",
    "- NLTK: This tool is handy for getting your data ready before fine-tuning, like breaking it into smaller pieces.<br>\n",
    "- IProgress: A Python library that provides a text progress bar for long-running operations.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface-hub==0.23.4\n",
    "%pip install transformers[torch]==4.41.2\n",
    "%pip install datasets==2.20.0\n",
    "%pip install sentencepiece==0.2.0\n",
    "%pip install tokenizers==0.19.1\n",
    "%pip install evaluate==0.4.2\n",
    "%pip install rouge-score==0.1.2\n",
    "%pip install nltk==3.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize error reports, the logging level was set to only show errors. For more information, please see: https://huggingface.co/transformers/v3.1.0/main_classes/logging.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "\n",
    "There are multiple formats of FLAN-T5 models are available on Hugging Face, from small to extra-large models, and the bigger the model, the more parameters it has.\n",
    "\n",
    "- google/flan-t4-small: 80 million parameters, 300 MB memory required\n",
    "- google/flan-t4-base: 250 million parameters, 990 MB memory required\n",
    "- google/flan-t5-large: 780 million parameters, 1 GB memory required\n",
    "- google/flan-t5-xl: 3 billion parameters, 12 GB memory required\n",
    "- google/flan-t4-xxl: 11 billion parameters, 80 GB memory required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/flan-t5-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the model from the model directory and loads the tokenizer from the tokenizer directory.  Both the model and tokenizer are the same model. \n",
    "\n",
    "With the DataCollectorForSeq2Seq, a data collector is created to be used for the question-answering task: https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForSeq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, cache_dir=\"model/tokenizer\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, cache_dir=\"model/pretrained\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing before fine-tuning\n",
    "\n",
    "Perform an inferencing example to see how the base model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"Kubernetes management\"\n",
    "inputs = tokenizer(user_input, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Input: {user_input}\")\n",
    "print(f\"Answer: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the language model is outputing slop (https://www.cnet.com/tech/services-and-software/the-new-ai-buzzword-is-slop-and-its-messing-with-you-what-to-watch-out-for and https://www.nytimes.com/2024/06/11/style/ai-search-slop.html). <br> \n",
    "\n",
    "It is a nonsensical response.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data set\n",
    "\n",
    "Next you will load the csv with some sample data and create a dataset from it.  The csv has two columns: Input and Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path  = \"netapp_products.csv\" \n",
    "df = pd.read_csv(path, encoding=\"latin-1\")\n",
    "df = df.dropna()\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the first 5 entries in the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data set into 2: \n",
    "- 80% for the training set and \n",
    "- 20% for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Keys of tokenized dataset: {list(dataset['train'].features)}\")\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "dataset[\"train\"].save_to_disk(\"data/train\")\n",
    "dataset[\"test\"].save_to_disk(\"data/eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequences longer than the MAX_OUTPUT_LENGTH this will be truncated, sequences shorter will be padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lenghts = [len(x) for x in dataset[\"train\"][\"Input\"]]\n",
    "# take 85 percentile of max length for better utilization\n",
    "MAX_INPUT_LENGTH = int(np.percentile(input_lenghts, 85))\n",
    "print(f\"Max input length: {MAX_INPUT_LENGTH}\")\n",
    "\n",
    "\n",
    "target_lengths = [len(x) for x in dataset[\"train\"][\"Output\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "MAX_OUTPUT_LENGTH = int(np.percentile(target_lengths, 90))\n",
    "print(f\"Max output length: {MAX_OUTPUT_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting and tokenization\n",
    "\n",
    "During the inference mode, the process of calling the model will be in this format:\n",
    "\n",
    "“Match the NetApp product with the description: <USER_INPUT>”\n",
    "\n",
    "Where the <USER_INPUT> is the input statement the user provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prefix\n",
    "prefix = \"Match the NetApp product with the description: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the model knows to do this, we need to format the training data by adding the task as a prefix to the input sentences. This is done using a function called preprocess_function.\n",
    "\n",
    "Along with formatting, this function also takes care of tokenizing the inputs and outputs using another function called tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    # Add prefix to the sentences\n",
    "    inputs = [prefix + doc for doc in examples[\"Input\"]]\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer(inputs, \n",
    "                             max_length=MAX_INPUT_LENGTH, \n",
    "                             truncation=True)\n",
    "    \n",
    "    # Tokenize the output text and set labels\n",
    "    labels = tokenizer(examples[\"Output\"], \n",
    "                       max_length=MAX_OUTPUT_LENGTH, \n",
    "                       truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we’ve defined the function, we apply it to the entire dataset using another function called map. This helps us quickly and efficiently process all the data in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Strategy\n",
    "\n",
    "Before diving into the training process, it is better to identify the metrics to evaluate the overall performance of the fine-tuning.\n",
    "\n",
    "Good evaluation metrics are important in any deep learning and machine learning project to evaluate the performance of models, not only during training but also later in production.\n",
    "\n",
    "Two of the most common metrics to evaluate the performance of a text generation model are BLEU and ROUGE, and in this case, to evaluate the quality of an answer by comparing it to a reference answer.\n",
    "\n",
    "The focus of this tutorial is ROUGE, but this wikipedia article provides more information about the BLEU score: https://en.wikipedia.org/wiki/BLEU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is ROUGE score?\n",
    "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. Some key components of ROUGE for question-answering include:\n",
    "\n",
    "- ROUGE-L: Measures the longest common subsequence between the candidate and reference answers. This focuses on recall of the full text.\n",
    "- ROUGE-1, ROUGE-2, ROUGE-SU4: Compare unigram, bigram, 4-gram overlaps between candidate and reference. Focus on recall of key parts/chunks\n",
    "- Higher ROUGE scores generally indicate better performance for question answering. Scores close to or above 0.70+ are considered strong\n",
    "- When using this metric, processing like stemming, and removing stopwords can help improve the overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper function compute_metrics can help compute the underlying ROUGE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Replace -100 in labels with pad token id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode preds and labels\n",
    "    #decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True) for pred in preds]\n",
    "    #decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Split decoded preds and labels into sentences\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    # Compute metrics\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To trigger the fine-tuning, we need to set some hyperparameters and the main ones are given below:\n",
    "\n",
    "- Learning rate: to control how quickly the model learns from the data and the typical values are 1e-5 to 5e-5, and for this use case, the value is set to 3e-4\n",
    "- Batch size: the total number of samples processed before the update of the model’s weights. Using larger batches can speed up the process, but the downside is that it can lead to poor performance. We use 8 for this use case\n",
    "- Per device train batch size: this one is similar to batch size, but it is specified per each device (GPU)\n",
    "- Weight decay: the goal of using this is to prevent the model from overfitting. 0.01 is an acceptable value for weight size\n",
    "- Save total limit: this is the total number of checkpoints to be saved during the training. The more saves there are, the higher the possibilities there are to roll back but uses more disk. We are performing 3 saves for this case\n",
    "- Number of epochs: the total number of passes through the training dataset. The more epochs, the longer the training time, but could also improve the model performance. Typically, a value from 3 to 10 is chosen, and 3 is used for this use case.\n",
    "\n",
    "For more information about the arguments, please see: https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.\n",
    "\n",
    "The above parameters are defined below, and used for setting up the model training arguments, and the overall training artifacts are saved in the folder results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global parameters\n",
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_EPOCHS = 6\n",
    "FINETUNEDMODEL = \"model/finetuned\"\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=FINETUNEDMODEL,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=L_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    predict_with_generate=True,\n",
    "    push_to_hub=False,\n",
    "    generation_max_length=MAX_INPUT_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the trainer is set up to trigger the training process of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_dataset[\"train\"],\n",
    "   eval_dataset=tokenized_dataset[\"test\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and save the model.  This will take about 8 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(FINETUNEDMODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing after fine-tuning\n",
    "\n",
    "Load the fine-tuned model and run a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = T5ForConditionalGeneration.from_pretrained(FINETUNEDMODEL)\n",
    "tokenizer = T5Tokenizer.from_pretrained(FINETUNEDMODEL)\n",
    "\n",
    "user_input = \"Kubernetes management\"\n",
    "inputs = tokenizer(user_input, return_tensors=\"pt\")\n",
    "outputs = finetuned_model.generate(**inputs, max_new_tokens=MAX_INPUT_LENGTH)\n",
    "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Input: {user_input}\")\n",
    "print(f\"Answer: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the fine-tuned model should be able to correctly match the correct description with the NetApp product."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
