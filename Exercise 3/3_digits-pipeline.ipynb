{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer Data Pipeline Notebook\n",
    "\n",
    "In this [Kaggle competition](https://www.kaggle.com/competitions/digit-recognizer/overview) \n",
    "\n",
    ">MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n",
    "\n",
    ">In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary packages\n",
    "\n",
    "We use the requirement.txt file to list all the dependencies and then run pip install for the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: termcolor is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  DEPRECATION: func-timeout is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  DEPRECATION: wrapt is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  DEPRECATION: fire is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  DEPRECATION: strip-hints is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  DEPRECATION: kfp-server-api is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  DEPRECATION: kfp is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt --user --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is the first time running this pip command, restart the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "In this section, we import the packages needed in this example.  It is good practice to gather your imports into a single place.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as components\n",
    "from typing import NamedTuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline variables and set default values\n",
    "user_namespace: str = \"kubeflow-user-example-com\"\n",
    "\n",
    "clone_step_container_image: str = \"curtisab/ndot-jupyter-scipy:v1alpha1\"\n",
    "clone_step_train_pvc_existing: str = \"digits-train\"\n",
    "clone_step_valid_pvc_existing: str = \"digits-valid\"\n",
    "\n",
    "shape_step_container_image: str = \"curtisab/ndot-jupyter-scipy:v1alpha1\"\n",
    "shape_step_train_pvc: str = \"digits-train-clone\"\n",
    "shape_step_train_mountpoint: str = \"/mnt/train\"\n",
    "shape_step_valid_pvc: str = \"digits-valid-clone\"\n",
    "shape_step_valid_mountpoint: str = \"/mnt/valid\"\n",
    "\n",
    "train_step_container_image: str = \"curtisab/ndot-jupyter-scipy:v1alpha1\"\n",
    "train_step_train_pvc: str = \"digits-train-clone\"\n",
    "train_step_train_mountpoint: str = \"/mnt/train\"\n",
    "train_step_valid_pvc: str = \"digits-valid-clone\"\n",
    "train_step_valid_mountpoint: str = \"/mnt/valid\"\n",
    "train_step_model_pvc_existing: str = \"digits-model\"\n",
    "train_step_model_mountpoint: str = \"/mnt/model\"\n",
    "\n",
    "serve_step_container_image: str = \"curtisab/ndot-jupyter-scipy:v1alpha1\"\n",
    "serve_step_model_pvc_existing: str = \"digits-model\"\n",
    "serve_step_model_mountpoint: str = \"/mnt/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU limits; Due to SDK limitations, this must be hardcoded\n",
    "train_step_num_gpu = 0\n",
    "valid_step_num_gpu = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone the data volumes\n",
    "This step will run in separate container that will execute the clone volume step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_step():\n",
    "    print(\"Data Clone Step\")\n",
    "    \n",
    "    \"\"\"\n",
    "    Clone the existing volumes\n",
    "    Export clone pvc name\n",
    "    \"\"\"\n",
    "\n",
    "    from netapp_dataops.k8s import clone_volume\n",
    "    \n",
    "    clone_volume(sourcePvcName=clone_step_train_pvc_existing, newPvcName=train_step_train_pvc, namespace=user_namespace)\n",
    "    clone_volume(sourcePvcName=clone_step_valid_pvc_existing, newPvcName=train_step_valid_pvc, namespace=user_namespace)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "In this component, this code will run in a containerized enviornment.  It will process the data and as it back to the persistent volume claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_step() :\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "\n",
    "    DATA_TRAIN_FILE = os.path.join(shape_step_train_mountpoint,'train.csv')\n",
    "    TRAIN_DF = pd.read_csv(DATA_TRAIN_FILE)\n",
    "    TRAIN_X = TRAIN_DF.drop('label', axis=1)\n",
    "    TRAIN_Y = TRAIN_DF.label\n",
    "    # Reshape image in 3 dimensions (height = 28px, width = 28px , channel = 1)... This is needed for the Keras API\n",
    "    TRAIN_X = TRAIN_X.values.reshape(-1,28,28,1)\n",
    "    # Normalize the data\n",
    "    # Each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n",
    "    TRAIN_X = TRAIN_X /255.0\n",
    "    DATA_TRAIN_X_FILE = os.path.join(shape_step_train_mountpoint, \"train_x.npy\")\n",
    "    np.save(DATA_TRAIN_X_FILE, TRAIN_X)\n",
    "    DATA_TRAIN_Y_FILE = os.path.join(shape_step_train_mountpoint, \"train_y.npy\")\n",
    "    np.save(DATA_TRAIN_Y_FILE, TRAIN_Y)\n",
    "\n",
    "    DATA_VALID_FILE = os.path.join(shape_step_valid_mountpoint,'valid.csv')\n",
    "    VALID_DF = pd.read_csv(DATA_VALID_FILE)\n",
    "    VALID_X = VALID_DF.drop('label', axis=1)\n",
    "    VALID_Y = VALID_DF.label\n",
    "    # Reshape image in 3 dimensions (height = 28px, width = 28px , channel = 1)... This is needed for the Keras API\n",
    "    VALID_X = VALID_X.values.reshape(-1,28,28,1)\n",
    "    # Normalize the data\n",
    "    # Each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n",
    "    VALID_X = VALID_X /255.0 \n",
    "    DATA_VALID_X_FILE = os.path.join(shape_step_valid_mountpoint, \"valid_x.npy\")\n",
    "    np.save(DATA_VALID_X_FILE, VALID_X)\n",
    "    DATA_VALID_Y_FILE = os.path.join(shape_step_valid_mountpoint, \"valid_y.npy\")\n",
    "    np.save(DATA_VALID_Y_FILE, VALID_Y)\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model generation step\n",
    "This step will execute in a separate container.  It will save the model to the model persistent volume claim.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Administrator.DEMO\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Administrator.DEMO/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def train_step(    \n",
    "    no_epochs:int = 1,\n",
    "    optimizer: str = \"adam\"\n",
    ") -> NamedTuple('Output', [('mlpipeline_ui_metadata', 'UI_metadata'),('mlpipeline_metrics', 'Metrics')]):\n",
    "\n",
    "    print(\"Model Generation Step\")\n",
    "\n",
    "    \"\"\"\n",
    "    Build the model with Keras API\n",
    "    Export model parameters\n",
    "    \"\"\"\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    # Construct the model structure\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(10, activation='softmax')) # Output are 10 classes, numbers from 0-9\n",
    "\n",
    "    # Show model summary - how it looks\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    metric_model_summary = \"\\n\".join(stringlist)\n",
    "    \n",
    "    # Compile the model - we want to have a binary outcome\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Get the data\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_train\",\"/tmp/x_train.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"y_train\",\"/tmp/y_train.npy\")\n",
    "    y_train = np.load(\"/tmp/y_train.npy\")\n",
    "    \n",
    "    # Fit the model and return the history while training\n",
    "    history = model.fit(\n",
    "      x=x_train,\n",
    "      y=y_train,\n",
    "      epochs=no_epochs,\n",
    "      batch_size=20,\n",
    "    )\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_test\",\"/tmp/x_test.npy\")\n",
    "    x_test = np.load(\"/tmp/x_test.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"y_test\",\"/tmp/y_test.npy\")\n",
    "    y_test = np.load(\"/tmp/y_test.npy\")\n",
    "    \n",
    "\n",
    "    # Test the model against the test dataset\n",
    "    # Returns the loss value & metrics values for the model in test mode.\n",
    "    model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "\n",
    "    # Generates output predictions for the input samples.\n",
    "    test_predictions = model.predict(x=x_test)\n",
    "\n",
    "    # Returns the indices of the maximum values along an axis.\n",
    "    test_predictions = np.argmax(test_predictions,axis=1) # the prediction outputs 10 values, we take the index number of the highest value, which is the prediction of the model\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    confusion_matrix = tf.math.confusion_matrix(labels=y_test,predictions=test_predictions)\n",
    "    confusion_matrix = confusion_matrix.numpy()\n",
    "    vocab = list(np.unique(y_test))\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(confusion_matrix):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "    cm_csv = df_cm.to_csv(header=False, index=False)\n",
    "    \n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"schema\": [\n",
    "                    {'name': 'target', 'type': 'CATEGORY'},\n",
    "                    {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                    {'name': 'count', 'type': 'NUMBER'},\n",
    "                  ],\n",
    "                \"target_col\" : \"actual\",\n",
    "                \"predicted_col\" : \"predicted\",\n",
    "                \"source\": cm_csv,\n",
    "                \"storage\": \"inline\",\n",
    "                \"labels\": [0,1,2,3,4,5,6,7,8,9]\n",
    "            },\n",
    "            {\n",
    "                'storage': 'inline',\n",
    "                'source': '''# Model Overview\n",
    "## Model Summary\n",
    "\n",
    "```\n",
    "{}\n",
    "```\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "**Accuracy**: {}\n",
    "**Loss**: {}\n",
    "\n",
    "'''.format(metric_model_summary,model_accuracy,model_loss),\n",
    "                'type': 'markdown',\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'model_accuracy',\n",
    "          'numberValue':  float(model_accuracy),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_loss',\n",
    "          'numberValue':  float(model_loss),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        }]}\n",
    "    \n",
    "    ### Save model to minIO\n",
    "    \n",
    "    keras.models.save_model(model,\"/tmp/detect-digits\")\n",
    "    \n",
    "    from minio import Minio\n",
    "    import os\n",
    "\n",
    "    minio_client = Minio(\n",
    "            \"100.65.11.110:9000\",\n",
    "            access_key=\"minio\",\n",
    "            secret_key=\"minio123\",\n",
    "            secure=False\n",
    "        )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "\n",
    "\n",
    "    import glob\n",
    "\n",
    "    def upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(\n",
    "                    minio_path, local_file[1 + len(local_path):])\n",
    "                remote_path = remote_path.replace(\n",
    "                    os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "\n",
    "    upload_local_directory_to_minio(\"/tmp/detect-digits\",minio_bucket,\"models/detect-digits/1/\") # 1 for version 1\n",
    "    \n",
    "    print(\"Saved model to minIO\")\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "    return output(json.dumps(metadata),json.dumps(metrics))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Administrator.DEMO\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Administrator.DEMO/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# def serve_step():\n",
    "#     print(\"Model Serve Step\")\n",
    "#     \"\"\"\n",
    "#     Create kserve instance\n",
    "#     \"\"\"\n",
    "#     from kubernetes import client \n",
    "#     from kserve import KServeClient\n",
    "#     from kserve import constants\n",
    "#     from kserve import utils\n",
    "#     from kserve import V1beta1InferenceService\n",
    "#     from kserve import V1beta1InferenceServiceSpec\n",
    "#     from kserve import V1beta1PredictorSpec\n",
    "#     from kserve import V1beta1TFServingSpec\n",
    "#     from datetime import datetime\n",
    "\n",
    "#     namespace = utils.get_default_target_namespace()\n",
    "\n",
    "#     now = datetime.now()\n",
    "#     v = now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "#     name='digits-recognizer-{}'.format(v)\n",
    "#     kserve_version='v1beta1'\n",
    "#     api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "#     isvc = V1beta1InferenceService(api_version=api_version,\n",
    "#                                    kind=constants.KSERVE_KIND,\n",
    "#                                    metadata=client.V1ObjectMeta(\n",
    "#                                        name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "#                                    spec=V1beta1InferenceServiceSpec(\n",
    "#                                    predictor=V1beta1PredictorSpec(\n",
    "#                                        tensorflow=(V1beta1TFServingSpec(\n",
    "#                                            storage_uri=\"pvc://\" + serve_step_model_pvc_existing)))) \n",
    "#                                             #QUESTION: Does this need to be something else\n",
    "#     )\n",
    "    \n",
    "#     KServe = KServeClient()\n",
    "#     KServe.create(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m comp_clone \u001b[39m=\u001b[39m components\u001b[39m.\u001b[39mcreate_component_from_func(clone_step, base_image\u001b[39m=\u001b[39mclone_step_container_image,\n\u001b[0;32m      3\u001b[0m                                                             packages_to_install\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mnetapp-dataops-k8s==2.4.0\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m comp_shape\u001b[39m=\u001b[39m components\u001b[39m.\u001b[39mfunc_to_container_op(shape_step, base_image\u001b[39m=\u001b[39mtrain_step_container_image)\n\u001b[1;32m----> 6\u001b[0m comp_shape\u001b[39m.\u001b[39;49mapply(\n\u001b[0;32m      7\u001b[0m     kfp\u001b[39m.\u001b[39monprem\u001b[39m.\u001b[39mmount_pvc(shape_step_train_pvc, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, shape_step_train_mountpoint)\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m comp_shape\u001b[39m.\u001b[39mapply(\n\u001b[0;32m     10\u001b[0m     kfp\u001b[39m.\u001b[39monprem\u001b[39m.\u001b[39mmount_pvc(shape_step_valid_pvc, \u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m, shape_step_valid_mountpoint)\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[39m# comp_train= components.create_component_from_func(train_step, base_image=train_step_container_image)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# comp_train.apply(\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m#     kfp.onprem.mount_pvc(train_step_train_pvc, 'train', train_step_train_mountpoint)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m#     kfp.onprem.mount_pvc(serve_step_model_pvc, 'model', serve_step_model_mountpoint)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m# )\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "# Generate components\n",
    "comp_clone = components.create_component_from_func(clone_step, base_image=clone_step_container_image,\n",
    "                                                            packages_to_install=['netapp-dataops-k8s==2.4.0'])\n",
    "\n",
    "comp_shape = components.func_to_container_op(shape_step, base_image=train_step_container_image)\n",
    "comp_shape.apply(\n",
    "    kfp.onprem.mount_pvc(shape_step_train_pvc, 'train', shape_step_train_mountpoint)\n",
    ")\n",
    "comp_shape.apply(\n",
    "    kfp.onprem.mount_pvc(shape_step_valid_pvc, 'valid', shape_step_valid_mountpoint)\n",
    ")\n",
    "\n",
    "# comp_train= components.create_component_from_func(train_step, base_image=train_step_container_image)\n",
    "# comp_train.apply(\n",
    "#     kfp.onprem.mount_pvc(train_step_train_pvc, 'train', train_step_train_mountpoint)\n",
    "# )\n",
    "# comp_train.apply(\n",
    "#     kfp.onprem.mount_pvc(train_step_model_pvc, 'model', train_step_model_mountpoint)\n",
    "# )\n",
    "\n",
    "# comp_serve= components.create_component_from_func(serve_step, base_image=serve_step_container_image,\n",
    "#                                                            packages_to_install=['kserve==0.10.1'])\n",
    "# comp_serve.apply(\n",
    "#     kfp.onprem.mount_pvc(serve_step_model_pvc, 'model', serve_step_model_mountpoint)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Administrator.DEMO\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Administrator.DEMO/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    name='digits-recognizer-pipeline',\n",
    "    description='Detect digits'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Administrator.DEMO\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Administrator.DEMO/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def create_pipe(no_epochs,optimizer):\n",
    "    step1 = comp_clone()\n",
    "    step2 = comp_shape()\n",
    "    step2.after(step1)\n",
    "    # step3 = comp_train(no_epochs,optimizer)\n",
    "    # step3.after(step2)\n",
    "    # step4 = comp_serve()\n",
    "    # step4.after(step3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Administrator.DEMO\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Administrator.DEMO/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    client = kfp.Client()\n",
    "\n",
    "    arguments = {\n",
    "        \"no_epochs\" : 1,\n",
    "        \"optimizer\": \"adam\"\n",
    "    }\n",
    "\n",
    "    now = datetime.now()\n",
    "    pipe_version = now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "    pipe_name = \"digits-pipe-\" + pipe_version\n",
    "    pipe_file = pipe_name + \".yaml\"\n",
    "    pipe_description = \"A sample digit recognizer pipeline\"\n",
    "\n",
    "    # Set this to 1 to run in Kubeflow instead of creating a yaml\n",
    "    run_directly = 0\n",
    "    \n",
    "    if (run_directly == 1):\n",
    "        client.create_run_from_pipeline_func(creaet_pipe,arguments=arguments,experiment_name=pipe_name)\n",
    "    else:\n",
    "        kfp.compiler.Compiler().compile(pipeline_func=create_pipe,package_path=pipe_file)\n",
    "        #client.upload_pipeline_version(pipeline_package_path=pipe_file,pipeline_version_name=pipe_version,pipeline_name=pipe_name,description=pipe_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Administrator.DEMO\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Administrator.DEMO/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aaec0c28839895f25e5089594964594e0b9ed4fc5fb3afe9a385b1f40747c44f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
