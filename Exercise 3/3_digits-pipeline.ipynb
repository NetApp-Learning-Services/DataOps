{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer Data Pipeline Notebook\n",
    "\n",
    "In this [Kaggle competition](https://www.kaggle.com/competitions/digit-recognizer/overview) \n",
    "\n",
    ">MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n",
    "\n",
    ">In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary packages\n",
    "\n",
    "We use the requirement.txt file to list all the dependencies and then run pip install for the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt --user --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is the first time running this pip command, restart the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "In this section, we import the packages needed in this example.  It is good practice to gather your imports into a single place.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as components\n",
    "from typing import NamedTuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline variables and set default values\n",
    "user_namespace: str = \"kubeflow-user-example-com\"\n",
    "\n",
    "clone_step_container_image: str = \"curtisab/ndot-jupyter-scipy:v1alpha1\"\n",
    "clone_step_train_pvc_existing: str = \"digits-train\"\n",
    "clone_step_valid_pvc_existing: str = \"digits-valid\"\n",
    "\n",
    "train_step_container_image: str = \"curtisab/ndot-jupyter-scipy:v1alpha1\"\n",
    "train_step_train_pvc: str = \"digits-train-clone\"\n",
    "train_step_train_mountpoint: str = \"/mnt/train\"\n",
    "train_step_valid_pvc: str = \"digits-valid-clone\"\n",
    "train_step_valid_mountpoint: str = \"/mnt/valid\"\n",
    "train_step_model_pvc_existing: str = \"digits-model\"\n",
    "train_step_model_mountpoint: str = \"/mnt/model\"\n",
    "\n",
    "\n",
    "serve_step_container_image: str = \"curtisab/ndot-jupyter-scipy:v1alpha1\"\n",
    "serve_step_model_pvc_existing: str = \"digits-model\"\n",
    "serve_step_model_mountpoint: str = \"/mnt/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU limits; Due to SDK limitations, this must be hardcoded\n",
    "train_step_num_gpu = 0\n",
    "valid_step_num_gpu = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone the data volumes\n",
    "This step will run in separate container that will execute the clone volume step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_step():\n",
    "    print(\"Data Clone Step\")\n",
    "    \n",
    "    \"\"\"\n",
    "    Clone the existing volumes\n",
    "    Export clone pvc name\n",
    "    \"\"\"\n",
    "\n",
    "    from netapp_dataops.k8s import clone_volume\n",
    "    \n",
    "    clone_volume(sourcePvcName=clone_step_train_pvc_existing, newPvcName=train_step_train_pvc, namespace=user_namespace)\n",
    "    clone_volume(sourcePvcName=clone_step_valid_pvc_existing, newPvcName=train_step_valid_pvc, namespace=user_namespace)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model generation step\n",
    "This step will execute in a separate container.  It will save the model to the model persistent volume claim.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(    \n",
    "    no_epochs:int = 1,\n",
    "    optimizer: str = \"adam\"\n",
    ") -> NamedTuple('Output', [('mlpipeline_ui_metadata', 'UI_metadata'),('mlpipeline_metrics', 'Metrics')]):\n",
    "\n",
    "    print(\"Model Generation Step\")\n",
    "\n",
    "    \"\"\"\n",
    "    Build the model with Keras API\n",
    "    Export model parameters\n",
    "    \"\"\"\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    # Construct the model structure\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(10, activation='softmax')) # Output are 10 classes, numbers from 0-9\n",
    "\n",
    "    # Show model summary - how it looks\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    metric_model_summary = \"\\n\".join(stringlist)\n",
    "    \n",
    "    # Compile the model - we want to have a binary outcome\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Get the data\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_train\",\"/tmp/x_train.npy\")\n",
    "    x_train = np.load(\"/tmp/x_train.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"y_train\",\"/tmp/y_train.npy\")\n",
    "    y_train = np.load(\"/tmp/y_train.npy\")\n",
    "    \n",
    "    # Fit the model and return the history while training\n",
    "    history = model.fit(\n",
    "      x=x_train,\n",
    "      y=y_train,\n",
    "      epochs=no_epochs,\n",
    "      batch_size=20,\n",
    "    )\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"x_test\",\"/tmp/x_test.npy\")\n",
    "    x_test = np.load(\"/tmp/x_test.npy\")\n",
    "    \n",
    "    minio_client.fget_object(minio_bucket,\"y_test\",\"/tmp/y_test.npy\")\n",
    "    y_test = np.load(\"/tmp/y_test.npy\")\n",
    "    \n",
    "\n",
    "    # Test the model against the test dataset\n",
    "    # Returns the loss value & metrics values for the model in test mode.\n",
    "    model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "\n",
    "    # Generates output predictions for the input samples.\n",
    "    test_predictions = model.predict(x=x_test)\n",
    "\n",
    "    # Returns the indices of the maximum values along an axis.\n",
    "    test_predictions = np.argmax(test_predictions,axis=1) # the prediction outputs 10 values, we take the index number of the highest value, which is the prediction of the model\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    confusion_matrix = tf.math.confusion_matrix(labels=y_test,predictions=test_predictions)\n",
    "    confusion_matrix = confusion_matrix.numpy()\n",
    "    vocab = list(np.unique(y_test))\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(confusion_matrix):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "    cm_csv = df_cm.to_csv(header=False, index=False)\n",
    "    \n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"schema\": [\n",
    "                    {'name': 'target', 'type': 'CATEGORY'},\n",
    "                    {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                    {'name': 'count', 'type': 'NUMBER'},\n",
    "                  ],\n",
    "                \"target_col\" : \"actual\",\n",
    "                \"predicted_col\" : \"predicted\",\n",
    "                \"source\": cm_csv,\n",
    "                \"storage\": \"inline\",\n",
    "                \"labels\": [0,1,2,3,4,5,6,7,8,9]\n",
    "            },\n",
    "            {\n",
    "                'storage': 'inline',\n",
    "                'source': '''# Model Overview\n",
    "## Model Summary\n",
    "\n",
    "```\n",
    "{}\n",
    "```\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "**Accuracy**: {}\n",
    "**Loss**: {}\n",
    "\n",
    "'''.format(metric_model_summary,model_accuracy,model_loss),\n",
    "                'type': 'markdown',\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'model_accuracy',\n",
    "          'numberValue':  float(model_accuracy),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_loss',\n",
    "          'numberValue':  float(model_loss),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        }]}\n",
    "    \n",
    "    ### Save model to minIO\n",
    "    \n",
    "    keras.models.save_model(model,\"/tmp/detect-digits\")\n",
    "    \n",
    "    from minio import Minio\n",
    "    import os\n",
    "\n",
    "    minio_client = Minio(\n",
    "            \"100.65.11.110:9000\",\n",
    "            access_key=\"minio\",\n",
    "            secret_key=\"minio123\",\n",
    "            secure=False\n",
    "        )\n",
    "    minio_bucket = \"mlpipeline\"\n",
    "\n",
    "\n",
    "    import glob\n",
    "\n",
    "    def upload_local_directory_to_minio(local_path, bucket_name, minio_path):\n",
    "        assert os.path.isdir(local_path)\n",
    "\n",
    "        for local_file in glob.glob(local_path + '/**'):\n",
    "            local_file = local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n",
    "            if not os.path.isfile(local_file):\n",
    "                upload_local_directory_to_minio(\n",
    "                    local_file, bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n",
    "            else:\n",
    "                remote_path = os.path.join(\n",
    "                    minio_path, local_file[1 + len(local_path):])\n",
    "                remote_path = remote_path.replace(\n",
    "                    os.sep, \"/\")  # Replace \\ with / on Windows\n",
    "                minio_client.fput_object(bucket_name, remote_path, local_file)\n",
    "\n",
    "    upload_local_directory_to_minio(\"/tmp/detect-digits\",minio_bucket,\"models/detect-digits/1/\") # 1 for version 1\n",
    "    \n",
    "    print(\"Saved model to minIO\")\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "    return output(json.dumps(metadata),json.dumps(metrics))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serve_step():\n",
    "    print(\"Model Serve Step\")\n",
    "    \"\"\"\n",
    "    Create kserve instance\n",
    "    \"\"\"\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TFServingSpec\n",
    "    from datetime import datetime\n",
    "\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "\n",
    "    now = datetime.now()\n",
    "    v = now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "    name='digits-recognizer-{}'.format(v)\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       tensorflow=(V1beta1TFServingSpec(\n",
    "                                           storage_uri=\"pvc://\" + serve_step_model_pvc_existing)))) \n",
    "                                            #QUESTION: Does this need to be something else\n",
    "    )\n",
    "    \n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate components\n",
    "comp_clone = components.create_component_from_func(clone_step, base_image=clone_step_container_image,\n",
    "                                                            packages_to_install=['netapp-dataops-k8s==2.4.0'])\n",
    "\n",
    "comp_train= components.create_component_from_func(train_step, base_image=train_step_container_image, )\n",
    "comp_train.apply(\n",
    "    kfp.onprem.mount_pvc(train_step_train_pvc, 'train', train_step_train_mountpoint)\n",
    ")\n",
    "comp_train.apply(\n",
    "    kfp.onprem.mount_pvc(train_step_model_pvc, 'model', train_step_model_mountpoint)\n",
    ")\n",
    "\n",
    "comp_serve= components.create_component_from_func(serve_step, base_image=serve_step_container_image,\n",
    "                                                           packages_to_install=['kserve==0.10.1'])\n",
    "comp_serve.apply(\n",
    "    kfp.onprem.mount_pvc(serve_step_model_pvc, 'model', serve_step_model_mountpoint)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='digits-recognizer-pipeline',\n",
    "    description='Detect digits'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipe(no_epochs,optimizer):\n",
    "    step1 = comp_clone()\n",
    "    step2 = comp_train(no_epochs,optimizer)\n",
    "    step2.after(step1)\n",
    "    step3 = comp_serve()\n",
    "    step3.after(step2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/home/jovyan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate a root folder for the data\n",
    "DATA_DIR = 'data'\n",
    "DATA_ROOT = os.path.join(ROOT_DIR, DATA_DIR)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "assert os.path.exists(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data paths\n",
    "DATA_TRAIN_PVC = 'digits-train'\n",
    "DATA_TRAIN_ROOT = os.path.join(DATA_ROOT, DATA_TRAIN_PVC)\n",
    "os.makedirs(DATA_TRAIN_ROOT, exist_ok=True)\n",
    "assert os.path.exists(DATA_TRAIN_ROOT)\n",
    "DATA_TRAIN_FILE = os.path.join(DATA_TRAIN_ROOT,'train.csv')\n",
    "print(DATA_TRAIN_FILE)\n",
    "assert os.path.exists(DATA_TRAIN_FILE)\n",
    "\n",
    "# Testing data paths\n",
    "DATA_TEST_PVC = 'digits-test'\n",
    "DATA_TEST_ROOT = os.path.join(DATA_ROOT, DATA_TEST_PVC)\n",
    "os.makedirs(DATA_TEST_ROOT, exist_ok=True)\n",
    "assert os.path.exists(DATA_TEST_ROOT)\n",
    "DATA_TEST_FILE = os.path.join(DATA_TEST_ROOT,'test.csv')\n",
    "assert os.path.exists(DATA_TEST_FILE)\n",
    "\n",
    "# Validation data paths\n",
    "DATA_VALID_PVC = 'digits-valid'\n",
    "DATA_VALID_ROOT = os.path.join(DATA_ROOT,DATA_VALID_PVC)\n",
    "os.makedirs(DATA_VALID_ROOT, exist_ok=True)\n",
    "assert os.path.exists(DATA_VALID_ROOT)\n",
    "DATA_VALID_FILE = os.path.join(DATA_VALID_ROOT,'valid.csv')\n",
    "assert os.path.exists(DATA_VALID_FILE)\n",
    "\n",
    "# Production data paths\n",
    "DATA_PROD_PVC = 'digits-prod'\n",
    "DATA_PROD_ROOT = os.path.join(DATA_ROOT, DATA_PROD_PVC)\n",
    "os.makedirs(DATA_PROD_ROOT, exist_ok=True)\n",
    "assert os.path.exists(DATA_PROD_ROOT)\n",
    "DATA_PROD_FILE = os.path.join(DATA_PROD_ROOT,'prod.csv')\n",
    "assert os.path.exists(DATA_PROD_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model data paths\n",
    "DATA_MODEL_PVC = 'digits-model'\n",
    "DATA_MODEL_ROOT = os.path.join(DATA_ROOT, DATA_MODEL_PVC)\n",
    "os.makedirs(DATA_MODEL_ROOT, exist_ok=True)\n",
    "assert os.path.exists(DATA_MODEL_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the data volumes\n",
    "We will not touch the original volumes but instead will work with cloned volumes only.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USER_NAMESPACE = \"kubeflow-user-example-com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New persistentvolumeclaims names for the clone volumes\n",
    "#CLONE_TRAIN_PVC = 'digits-train-clone' \n",
    "#CLONE_VALID_PVC = 'digits-valid-clone'\n",
    "#CLONE_TEST_PVC = 'digits-test-clone'\n",
    "#CLONE_PROD_PVC = 'digits-prod-clone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the training volume \n",
    "# in the USER_NAMESPACE namespace and create a new persistentvolumeclaim\n",
    "#cloneVolume(sourcePvcName=DATA_TRAIN_PVC, newPvcName=CLONE_TRAIN_PVC, namespace=USER_NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLONE_DIR = 'clone'\n",
    "#CLONE_ROOT = os.path.join(ROOT_DIR, CLONE_DIR)\n",
    "#os.makedirs(CLONE_ROOT, exist_ok=True)\n",
    "#assert os.path.exists(CLONE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount the new clone volume under the DATA_ROOT\n",
    "#import subprocess\n",
    "#nfs_server = \"192.168.0.71\"\n",
    "#nfs_export = \"/trident_pvc_a9abdc63_4840_493b_bdfe_45f2238dcc15\"\n",
    "#CLONE_TRAIN_ROOT = os.path.join(CLONE_ROOT, CLONE_TRAIN_PVC)\n",
    "#os.makedirs(CLONE_TRAIN_ROOT, exist_ok=True)\n",
    "#assert os.path.exists(CLONE_TRAIN_ROOT)\n",
    "\n",
    "#subprocess.run(['mount', '-t', 'nfs', f'{nfs_server}:{nfs_export}', CLONE_TRAIN_ROOT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the validation volume \n",
    "# in the USER_NAMESPACE namespace and create a new persistentvolumeclaim\n",
    "#cloneVolume  --source-pvc-name=DATA_VALID_PVC --new-pvc-name=CLONE_VALID_PVC --namespace=USER_NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the test volume \n",
    "# in the USER_NAMESPACE namespace and create a new persistentvolumeclaim\n",
    "#cloneVolume  --source-pvc-name=DATA_TEST_PVC --new-pvc-name=CLONE_TEST_PVC --namespace=USER_NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the production volume \n",
    "# in the USER_NAMESPACE namespace and create a new persistentvolumeclaim\n",
    "#cloneVolume  --source-pvc-name=DATA_PROD_PVC --new-pvc-name=CLONE_PROD_PVC --namespace=USER_NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset into pandas \n",
    "TRAIN_DF = pd.read_csv(DATA_TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the top 5 rows of the training data\n",
    "TRAIN_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial shape of the training data\n",
    "TRAIN_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out the image data (_X) from the label (_Y) for the train set\n",
    "TRAIN_X = TRAIN_DF.drop('label', axis=1)\n",
    "TRAIN_Y = TRAIN_DF.label\n",
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , channel = 1)... This is needed for the Keras API\n",
    "TRAIN_X = TRAIN_X.values.reshape(-1,28,28,1)\n",
    "# Normalize the data\n",
    "# Each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n",
    "TRAIN_X = TRAIN_X /255.0\n",
    "TRAIN_X.shape, TRAIN_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set random seed for reproducibility and ignore warning messages\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a model using a stack of layers\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Creating 3 layers of a convolution network\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "# Flatting the results\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "# Creating output\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "# Most important\n",
    "# Output are 10 classes, numbers from 0-9\n",
    "model.add(keras.layers.Dense(10, activation='softmax')) \n",
    "\n",
    "# Show model summary - how it looks\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PNG_MODEL_FILE = os.path.join(ROOT_DIR,'digits-model')\n",
    "visualizer(model, file_name=PNG_MODEL_FILE, file_format='png', view=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(PNG_MODEL_FILE+ '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model - we want to have a multiple outcome\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and return the history while training\n",
    "history = model.fit(\n",
    "  x=TRAIN_X,\n",
    "  y=TRAIN_Y,\n",
    "  epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model the model volume\n",
    "keras.models.save_model(model, DATA_MODEL_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if already trained\n",
    "model = keras.models.load_model(DATA_MODEL_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_DF = pd.read_csv(DATA_VALID_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spilt the training data into so the label is in TRAIN_Y and TRAIN_X doesn't include the label\n",
    "VALID_X = VALID_DF.drop('label', axis=1)\n",
    "VALID_Y = VALID_DF.label\n",
    "\n",
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , channel = 1)\n",
    "VALID_X = VALID_X.values.reshape(-1,28,28,1)\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "# Each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n",
    "VALID_X = VALID_X / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model against the test dataset\n",
    "# Returns the loss value & metrics values for the model in test mode.\n",
    "model_loss, model_accuracy = model.evaluate(x=VALID_X,y=VALID_Y, verbose=0)\n",
    "print(\"Test_loss: {}, Test_accuracy: {} \".format(model_loss,model_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "# Generates output predictions for the input samples.\n",
    "test_predictions = model.predict(x=VALID_X)\n",
    "\n",
    "# Returns the indices of the maximum values along an axis.\n",
    "test_predictions = np.argmax(test_predictions,axis=1) # the prediction outputs 10 values, we take the index number of the highest value, which is the prediction of the model\n",
    "\n",
    "# generate confusion matrix\n",
    "confusion_matrix = tf.math.confusion_matrix(labels=VALID_Y,predictions=test_predictions)\n",
    "\n",
    "# plot confusion matrix\n",
    "h = sns.heatmap(confusion_matrix, fmt='g', cbar=False, annot=True,cmap='Blues')\n",
    "h.set(xlabel='Predicted', ylabel='Actual', title=\"Confusion Matrix\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a snapshot of the model volume\n",
    "\n",
    "Creating a snapshot of the model volume, allows for protection and also cloning of the volume in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_NAMESPACE = \"kubeflow-user-example-com\"\n",
    "DATA_MODEL_SNAP = 'digits-model-snap'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VolumeSnapshot for the volume attached to the \n",
    "#   PersistentVolumeClaim (PVC) named in the variable DATA_MODEL_PVC in namespace in USER_NAMESPACE.\n",
    "#   NOTE: if snapshotName is not specified, the snapshot name will be set to 'ntap-dsutil.<timestamp>\n",
    "createVolumeSnapshot(pvcName=DATA_MODEL_PVC, namespace=USER_NAMESPACE, snapshotName=DATA_MODEL_SNAP, printOutput=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the snapshots of the volumes used\n",
    "We will revert the volumes for the train and valid volumes to ensure that nothing has changed while the model creation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_SNAP = 'digits-train-snap'\n",
    "DATA_VALID_SNAP = 'digits-valid-snap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore a VolumeSnapshot for the volume attached \n",
    "restoreVolumeSnapshot(snapshoptName=DATA_TRAIN_SNAP, namespace=USER_NAMESPACE,  printOutput=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore a VolumeSnapshot for the volume attached \n",
    "restoreVolumeSnapshot(snapshoptName=DATA_VALID_SNAP, namespace=USER_NAMESPACE,  printOutput=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aaec0c28839895f25e5089594964594e0b9ed4fc5fb3afe9a385b1f40747c44f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
