{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer Data Pipeline Notebook\n",
    "\n",
    "In this [Kaggle competition](https://www.kaggle.com/competitions/digit-recognizer/overview) \n",
    "\n",
    ">MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n",
    "\n",
    ">In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Imports for the entire notebook\n",
    "\n",
    "from datetime import datetime\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.onprem as onprem\n",
    "import kfp.components as components\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Container images\n",
    "clone_step_container_image: str = \"curtisab/ndot-jupyter-scipy:v1alpha1\"\n",
    "shape_step_container_image: str = \"curtisab/ndot-jupyter-scipy:v1alpha1\"\n",
    "train_step_container_image: str = \"curtisab/ndot-jupyter-scipy:v1alpha1\"\n",
    "serve_step_container_image: str = \"public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Clone Volumes Step\n",
    "def clone_step(\n",
    "    user_namespace: str = \"kubeflow-user-example-com\",\n",
    "    clone_step_train_pvc_existing: str = \"digits-train\",\n",
    "    clone_step_valid_pvc_existing: str = \"digits-valid\",\n",
    "    clone_step_train_pvc: str = \"digits-train-clone\",\n",
    "    clone_step_valid_pvc: str = \"digits-valid-clone\"\n",
    "):\n",
    "    print(\"Data Clone Step\")\n",
    "    \n",
    "    \"\"\"\n",
    "    Clone the existing volumes\n",
    "    Export clone pvc name\n",
    "    \"\"\"\n",
    "\n",
    "    from netapp_dataops.k8s import clone_volume\n",
    "    \n",
    "    clone_volume(\n",
    "        source_pvc_name=clone_step_train_pvc_existing, \n",
    "        new_pvc_name=clone_step_train_pvc, \n",
    "        namespace=user_namespace, \n",
    "        print_output=True)\n",
    "    clone_volume(\n",
    "        source_pvc_name=clone_step_valid_pvc_existing, \n",
    "        new_pvc_name=clone_step_valid_pvc, \n",
    "        namespace=user_namespace, \n",
    "        print_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Shape Data Step\n",
    "def shape_step(\n",
    "    shape_step_train_mountpoint: str = \"/mnt/train\",\n",
    "    shape_step_valid_mountpoint: str = \"/mnt/valid\"\n",
    ") :\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    DATA_TRAIN_FILE = os.path.join(shape_step_train_mountpoint,'train.csv')\n",
    "    TRAIN_DF = pd.read_csv(DATA_TRAIN_FILE)\n",
    "    TRAIN_X = TRAIN_DF.drop('label', axis=1)\n",
    "    TRAIN_Y = TRAIN_DF.label\n",
    "    # Reshape image in 3 dimensions (height = 28px, width = 28px , channel = 1)... This is needed for the Keras API\n",
    "    TRAIN_X = TRAIN_X.values.reshape(-1,28,28,1)\n",
    "    # Normalize the data\n",
    "    # Each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n",
    "    TRAIN_X = TRAIN_X /255.0\n",
    "    DATA_TRAIN_X_FILE = os.path.join(shape_step_train_mountpoint, \"train_x.npy\")\n",
    "    np.save(DATA_TRAIN_X_FILE, TRAIN_X)\n",
    "    print('File saved: ' + DATA_TRAIN_X_FILE)\n",
    "    DATA_TRAIN_Y_FILE = os.path.join(shape_step_train_mountpoint, \"train_y.npy\")\n",
    "    np.save(DATA_TRAIN_Y_FILE, TRAIN_Y)\n",
    "    print('File saved: ' + DATA_TRAIN_Y_FILE)\n",
    "\n",
    "    DATA_VALID_FILE = os.path.join(shape_step_valid_mountpoint,'valid.csv')\n",
    "    VALID_DF = pd.read_csv(DATA_VALID_FILE)\n",
    "    VALID_X = VALID_DF.drop('label', axis=1)\n",
    "    VALID_Y = VALID_DF.label\n",
    "    # Reshape image in 3 dimensions (height = 28px, width = 28px , channel = 1)... This is needed for the Keras API\n",
    "    VALID_X = VALID_X.values.reshape(-1,28,28,1)\n",
    "    # Normalize the data\n",
    "    # Each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1\n",
    "    VALID_X = VALID_X /255.0 \n",
    "    DATA_VALID_X_FILE = os.path.join(shape_step_valid_mountpoint, \"valid_x.npy\")\n",
    "    np.save(DATA_VALID_X_FILE, VALID_X)\n",
    "    print('File saved: ' + DATA_VALID_X_FILE)\n",
    "    DATA_VALID_Y_FILE = os.path.join(shape_step_valid_mountpoint, \"valid_y.npy\")\n",
    "    np.save(DATA_VALID_Y_FILE, VALID_Y)\n",
    "    print('File saved: ' + DATA_VALID_Y_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model Step\n",
    "def train_step(    \n",
    "    no_epochs:int = 1,   \n",
    "    optimizer: str = \"adam\",\n",
    "    train_step_train_mountpoint: str = \"/mnt/train\",\n",
    "    train_step_valid_mountpoint: str = \"/mnt/valid\",\n",
    "    train_step_model_mountpoint: str = \"/mnt/model\"\n",
    ") -> NamedTuple('Output', [('mlpipeline_ui_metadata', 'UI_metadata'),('mlpipeline_metrics', 'Metrics')]):\n",
    "\n",
    "    print(\"Model Generation Step\")\n",
    "\n",
    "    \"\"\"\n",
    "    Build the model with Keras API\n",
    "    Export model parameters\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from tensorflow import keras\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    # Construct the model structure\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(keras.layers.MaxPool2D(2, 2))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(10, activation='softmax')) # Output are 10 classes, numbers from 0-9\n",
    "\n",
    "    # Show model summary - how it looks\n",
    "    stringlist = []\n",
    "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
    "    metric_model_summary = \"\\n\".join(stringlist)\n",
    "    \n",
    "    # Compile the model - we want to have a binary outcome\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Get the data\n",
    "    DATA_TRAIN_X_FILE = os.path.join(train_step_train_mountpoint, \"train_x.npy\")\n",
    "    x_train = np.load(DATA_TRAIN_X_FILE)\n",
    "    \n",
    "    DATA_TRAIN_Y_FILE = os.path.join(train_step_train_mountpoint, \"train_y.npy\")\n",
    "    y_train = np.load(DATA_TRAIN_Y_FILE)\n",
    "    \n",
    "    # Fit the model and return the history while training\n",
    "    history = model.fit(\n",
    "      x=x_train,\n",
    "      y=y_train,\n",
    "      epochs=no_epochs,\n",
    "      batch_size=20,\n",
    "    )\n",
    "    \n",
    "    DATA_VALID_X_FILE = os.path.join(train_step_valid_mountpoint, \"valid_x.npy\")\n",
    "    x_test = np.load(DATA_VALID_X_FILE)\n",
    "\n",
    "    DATA_VALID_Y_FILE = os.path.join(train_step_valid_mountpoint, \"valid_y.npy\")\n",
    "    y_test = np.load(DATA_VALID_Y_FILE)\n",
    "    \n",
    "\n",
    "    # Test the model against the test dataset\n",
    "    # Returns the loss value & metrics values for the model in test mode.\n",
    "    model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "\n",
    "    # Generates output predictions for the input samples.\n",
    "    test_predictions = model.predict(x=x_test)\n",
    "\n",
    "    # Returns the indices of the maximum values along an axis.\n",
    "    test_predictions = np.argmax(test_predictions,axis=1) # the prediction outputs 10 values, we take the index number of the highest value, which is the prediction of the model\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    confusion_matrix = tf.math.confusion_matrix(labels=y_test,predictions=test_predictions)\n",
    "    confusion_matrix = confusion_matrix.numpy()\n",
    "    vocab = list(np.unique(y_test))\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(confusion_matrix):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "    cm_csv = df_cm.to_csv(header=False, index=False)\n",
    "    \n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"schema\": [\n",
    "                    {'name': 'target', 'type': 'CATEGORY'},\n",
    "                    {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "                    {'name': 'count', 'type': 'NUMBER'},\n",
    "                  ],\n",
    "                \"target_col\" : \"actual\",\n",
    "                \"predicted_col\" : \"predicted\",\n",
    "                \"source\": cm_csv,\n",
    "                \"storage\": \"inline\",\n",
    "                \"labels\": [0,1,2,3,4,5,6,7,8,9]\n",
    "            },\n",
    "            {\n",
    "                'storage': 'inline',\n",
    "                'source': '''# Model Overview\n",
    "## Model Summary\n",
    "\n",
    "```\n",
    "{}\n",
    "```\n",
    "\n",
    "## Model Performance\n",
    "\n",
    "**Accuracy**: {}\n",
    "**Loss**: {}\n",
    "\n",
    "'''.format(metric_model_summary,model_accuracy,model_loss),\n",
    "                'type': 'markdown',\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'model_accuracy',\n",
    "          'numberValue':  float(model_accuracy),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        },{\n",
    "          'name': 'model_loss',\n",
    "          'numberValue':  float(model_loss),\n",
    "          'format' : \"PERCENTAGE\"\n",
    "        }]}\n",
    "    \n",
    "    ### Save model to the storage\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    DATA_MODEL_VERSION = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    DATA_MODEL_VERSION_PATH = os.path.join(train_step_model_mountpoint, DATA_MODEL_VERSION)\n",
    "    os.makedirs(DATA_MODEL_VERSION_PATH, exist_ok=True)\n",
    "    keras.models.save_model(model,DATA_MODEL_VERSION_PATH)\n",
    "\n",
    "    ### Save model to the version 1 folder\n",
    "    import shutil\n",
    "    DATA_MODEL_V1_PATH = os.path.join(train_step_model_mountpoint, \"1\")\n",
    "    if os.path.exists(DATA_MODEL_V1_PATH) and os.path.isdir(DATA_MODEL_V1_PATH):\n",
    "        # Delete the old version\n",
    "        shutil.rmtree(DATA_MODEL_V1_PATH) \n",
    "    # Recreate the version 1 folder\n",
    "    os.makedirs(DATA_MODEL_V1_PATH, exist_ok=False)\n",
    "    keras.models.save_model(model,DATA_MODEL_V1_PATH)\n",
    "    \n",
    "    print(\"Saved model to the model volume twice\")\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "    return output(json.dumps(metadata),json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serve Model Step\n",
    "def serve_step(\n",
    "    train_step_model_pvc_existing: str = \"digits-model\",\n",
    "    user_namespace: str = \"kubeflow-user-example-com\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create kserve instance\n",
    "    \"\"\"\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TFServingSpec\n",
    "    from datetime import datetime\n",
    "\n",
    "    print(\"Model Serve Step\")\n",
    "\n",
    "    namespace = user_namespace\n",
    "\n",
    "    # now = datetime.now()\n",
    "    # v = now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "    #name='digits-recognizer-{}'.format(v)\n",
    "    name='digits-recognizer'\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "    pvc_path = \"pvc://\" + train_step_model_pvc_existing \n",
    "    print(\"Path to pvc: \" + pvc_path)\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       tensorflow=(V1beta1TFServingSpec(\n",
    "                                           storage_uri=pvc_path))))\n",
    "    )\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create components from the step functions\n",
    "comp_clone = components.create_component_from_func(\n",
    "    clone_step, \n",
    "    base_image=clone_step_container_image,\n",
    "    packages_to_install=['netapp-dataops-k8s==2.4.0', 'kfp==1.8.20', 'jsonschema==4.17.3', 'requests==2.25.1'])\n",
    "\n",
    "comp_shape = components.func_to_container_op(\n",
    "    shape_step, \n",
    "    base_image=shape_step_container_image)\n",
    "\n",
    "comp_train= components.create_component_from_func(\n",
    "    train_step, \n",
    "    base_image=train_step_container_image, \n",
    "    packages_to_install=['tensorflow==2.12.0'])\n",
    "\n",
    "comp_serve = components.create_component_from_func(\n",
    "    serve_step,\n",
    "    base_image=serve_step_container_image,\n",
    "    packages_to_install=['kserve==0.10.1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pipeline Metadata\n",
    "@dsl.pipeline(\n",
    "    name='digits-recognizer-pipeline',\n",
    "    description='Detect digits'\n",
    ")\n",
    "# Main Method To Construct the Pipeline\n",
    "def create_pipe(\n",
    "    no_epochs: int = 1,\n",
    "    optimizer = \"adam\",\n",
    "    user_namespace = \"kubeflow-user-example-com\",\n",
    "    clone_step_train_pvc_existing = \"digits-train\",\n",
    "    clone_step_valid_pvc_existing = \"digits-valid\",\n",
    "    clone_step_train_pvc = \"digits-train-clone\",\n",
    "    clone_step_valid_pvc= \"digits-valid-clone\",\n",
    "    shape_step_train_mountpoint = \"/mnt/train\",\n",
    "    shape_step_valid_mountpoint = \"/mnt/valid\",\n",
    "    train_step_train_mountpoint = \"/mnt/train\",\n",
    "    train_step_valid_mountpoint = \"/mnt/valid\",\n",
    "    train_step_model_mountpoint = \"/mnt/model\",\n",
    "    train_step_model_pvc_existing = \"digits-model\",\n",
    "    serve_step_model_pvc_existing = \"digits-model\",\n",
    "):\n",
    "\n",
    "\n",
    "    step1 = comp_clone(\n",
    "        user_namespace,\n",
    "        clone_step_train_pvc_existing,\n",
    "        clone_step_valid_pvc_existing,\n",
    "        clone_step_train_pvc,\n",
    "        clone_step_valid_pvc)\n",
    "    step2 = comp_shape(\n",
    "        shape_step_train_mountpoint, \n",
    "        shape_step_valid_mountpoint)\n",
    "    step2.apply(\n",
    "        onprem.mount_pvc(clone_step_train_pvc, 'train', shape_step_train_mountpoint)\n",
    "    )\n",
    "    step2.apply(\n",
    "        onprem.mount_pvc(clone_step_valid_pvc, 'valid', shape_step_valid_mountpoint)\n",
    "    )\n",
    "    step2.after(step1)\n",
    "\n",
    "    step3 = comp_train(\n",
    "        no_epochs, \n",
    "        optimizer, \n",
    "        train_step_train_mountpoint, \n",
    "        train_step_valid_mountpoint, \n",
    "        train_step_model_mountpoint)\n",
    "    step3.apply(\n",
    "        onprem.mount_pvc(clone_step_train_pvc, 'train', train_step_train_mountpoint)\n",
    "    )\n",
    "    step3.apply(\n",
    "        onprem.mount_pvc(clone_step_valid_pvc, 'valid', train_step_valid_mountpoint)\n",
    "    )\n",
    "    step3.apply(\n",
    "        onprem.mount_pvc(train_step_model_pvc_existing, 'model', train_step_model_mountpoint)\n",
    "    )\n",
    "    step3.after(step2)\n",
    "    step4 = comp_serve(\n",
    "        serve_step_model_pvc_existing, \n",
    "        user_namespace)\n",
    "    step4.after(step3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MAIN Function That Runs All Previous Code\n",
    "if __name__ == \"__main__\":\n",
    "    client = kfp.Client()\n",
    "\n",
    "    arguments = {\n",
    "        \"no_epochs\" : 1,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"user_namespace\": \"kubeflow-user-example-com\",\n",
    "        \"clone_step_train_pvc_existing\": \"digits-train\",\n",
    "        \"clone_step_valid_pvc_existing\": \"digits-valid\",\n",
    "        \"clone_step_train_pvc\": \"digits-train-clone\",\n",
    "        \"clone_step_valid_pvc\": \"digits-valid-clone\",\n",
    "        \"shape_step_train_mountpoint\":  \"/mnt/train\",\n",
    "        \"shape_step_valid_mountpoint\": \"/mnt/valid\",\n",
    "        \"train_step_train_pvc_existing\": \"digits-model\",\n",
    "        \"train_step_train_mountpoint\":  \"/mnt/train\",\n",
    "        \"train_step_valid_mountpoint\": \"/mnt/valid\",\n",
    "        \"serve_step_train_pvc_existing\": \"digits-model\",\n",
    "    }\n",
    "\n",
    "    now = datetime.now()\n",
    "    pipe_version = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    pipe_name = \"digits-pipe-\" + pipe_version\n",
    "    pipe_file = pipe_name + \".yaml\"\n",
    "    pipe_description = \"A sample digit recognizer pipeline\"\n",
    "\n",
    "    # Set this to 1 to run in Kubeflow instead of creating a yaml\n",
    "    run_directly = 0\n",
    "    \n",
    "    if (run_directly == 1):\n",
    "        client.create_run_from_pipeline_func(create_pipe,arguments=arguments,experiment_name=pipe_name)\n",
    "    else:\n",
    "        kfp.compiler.Compiler().compile(pipeline_func=create_pipe,package_path=pipe_file)\n",
    "        #client.upload_pipeline_version(pipeline_package_path=pipe_file,pipeline_version_name=pipe_version,pipeline_name=pipe_name,description=pipe_description)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
